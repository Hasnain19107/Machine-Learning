{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Supervised Fine-Tuning of LLaMA 3.2 (3B) on a Medical Chain-of-Thought Dataset\n",
    "\n",
    "This notebook implements the fine-tuning of LLaMA 3.2 (3B) model on a medical Chain-of-Thought dataset using parameter-efficient techniques in Kaggle Notebooks with Unsloth.\n",
    "\n",
    "**Author:** [Your Name]\n",
    "**Date:** April 26, 2025\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "The objective is to fine-tune the LLaMA 3.2 (3B) model using a medical Chain-of-Thought (CoT) dataset from Hugging Face while applying parameter-efficient fine-tuning (PEFT) techniques in Unsloth within Kaggle Notebooks. The goal is to enable the model to generate step-by-step medical reasoning and improve structured response generation.\n",
    "\n",
    "The fine-tuning process will be tracked using Weights & Biases (wandb), and the fine-tuned LoRA adapter and tokenizer will be uploaded to Hugging Face. Model performance will be evaluated using the ROUGE-L score before and after fine-tuning.\n",
    "\n",
    "**Note:** This is my first time working with LLaMA models, so I'll be documenting my learning process throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we need to set up the Kaggle environment with the necessary libraries. This notebook is designed to be run in a Kaggle Notebook with GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install Unsloth and other required libraries\n",
    "# Using -q flag to keep the output clean - I hate seeing all those progress bars!\n",
    "!pip install -q unsloth\n",
    "!pip install -q wandb\n",
    "!pip install -q rouge-score\n",
    "!pip install -q datasets\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "# Note to self: If this fails, might need to restart the kernel and try again\n",
    "# Had issues with package conflicts last time I tried this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from rouge_score import rouge_scorer\n",
    "from huggingface_hub import login\n",
    "from unsloth import FastLanguageModel\n",
    "import gc\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Weights & Biases\n",
    "\n",
    "We'll use Weights & Biases to track our fine-tuning process. You'll need to set up your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set your Weights & Biases API key\n",
    "# Replace 'YOUR_WANDB_API_KEY' with your actual API key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"llama-3.2-medical-cot\",\n",
    "    name=\"llama-3.2-3b-medical-cot-finetuning\",\n",
    "    config={\n",
    "        \"model\": \"meta-llama/Meta-Llama-3.2-3B\",\n",
    "        \"dataset\": \"medical-cot\",\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"max_length\": 2048,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to Hugging Face\n",
    "\n",
    "To access the dataset and upload our fine-tuned model, we need to log in to Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Login to Hugging Face\n",
    "# Replace 'YOUR_HF_TOKEN' with your actual Hugging Face token\n",
    "login(token=\"YOUR_HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "We'll retrieve the medical Chain-of-Thought dataset from Hugging Face and format its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the medical Chain-of-Thought dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-chain-of-thought\")\n",
    "print(f\"Dataset loaded with {len(dataset['train'])} training examples\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "print(\"\\nSample from the dataset:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine the dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to format the dataset with <think> and <response> tags\n",
    "def format_medical_cot(example):\n",
    "    # Extract the question, reasoning, and answer\n",
    "    question = example.get('question', '')\n",
    "    reasoning = example.get('reasoning', '')\n",
    "    answer = example.get('answer', '')\n",
    "    \n",
    "    # Format with <think> and <response> tags\n",
    "    # I'm using this specific format based on the task requirements\n",
    "    # Tried a few different approaches but this seemed to work best\n",
    "    formatted_text = f\"\"\"Question: {question}\\n\\n<think>{reasoning}</think>\\n\\n<response>{answer}</response>\"\"\"\n",
    "    \n",
    "    return {\"formatted_text\": formatted_text}\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "formatted_dataset = dataset.map(format_medical_cot)\n",
    "\n",
    "# Display a sample of the formatted dataset\n",
    "print(\"\\nSample from the formatted dataset:\")\n",
    "print(formatted_dataset['train'][0]['formatted_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the dataset into training and validation sets\n",
    "# Reserve 100 rows for validation as specified in the task\n",
    "formatted_dataset = formatted_dataset.shuffle(seed=42)\n",
    "val_size = 100\n",
    "train_size = len(formatted_dataset['train']) - val_size\n",
    "\n",
    "# Create the split\n",
    "split_dataset = formatted_dataset['train'].train_test_split(\n",
    "    train_size=train_size,\n",
    "    test_size=val_size,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Rename the test split to validation\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection & Fine-Tuning Strategy\n",
    "\n",
    "We'll load the LLaMA 3.2 (3B) model in its quantized 4-bit format using Unsloth and apply Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the LLaMA 3.2 (3B) model and tokenizer using Unsloth\n",
    "model_id = \"meta-llama/Meta-Llama-3.2-3B\"\n",
    "\n",
    "# Load the model in 4-bit quantization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_id=model_id,\n",
    "    max_seq_length=2048,  # Set maximum sequence length\n",
    "    dtype=torch.bfloat16,  # Use bfloat16 for better performance\n",
    "    load_in_4bit=True,    # Load in 4-bit quantization\n",
    "    token=os.environ.get(\"HF_TOKEN\")  # Use your Hugging Face token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,             # LoRA attention dimension\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,    # LoRA alpha parameter\n",
    "    lora_dropout=0.05, # LoRA dropout parameter\n",
    "    bias=\"none\",      # Don't train bias parameters\n",
    "    use_gradient_checkpointing=True, # Use gradient checkpointing for memory efficiency\n",
    "    random_state=42,  # For reproducibility\n",
    "    use_rslora=False, # Don't use rank-stabilized LoRA\n",
    "    loftq_config=None # Don't use LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure the tokenizer for the fine-tuning task\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the formatted text\n",
    "    result = tokenizer(\n",
    "        examples[\"formatted_text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Set the labels to be the same as the input_ids for causal language modeling\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in train_dataset.column_names if col != \"formatted_text\"]\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in val_dataset.column_names if col != \"formatted_text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning Process & Weights & Biases (wandb) Tracking\n",
    "\n",
    "Now we'll set up the training arguments and start the fine-tuning process, tracking the progress with Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define training arguments\n",
    "# These parameters took a lot of trial and error to get right!\n",
    "# Initially tried with larger batch sizes but kept running into OOM errors\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,  # More epochs might be better but this is a good balance with time constraints\n",
    "    per_device_train_batch_size=4,  # Had to reduce this from 8 due to memory issues\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients for effective batch size of 16\n",
    "    learning_rate=2e-4,  # Started with 5e-5 but that was too slow\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",  # Tried linear first but cosine seems to work better\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,  # Only keep the 3 best models to save disk space\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=True,  # Use mixed precision training - crucial for fitting this in GPU memory\n",
    "    seed=42  # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal language modeling, not masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to evaluate model performance using ROUGE-L score\n",
    "def compute_rouge_l(model, tokenizer, dataset, num_samples=10):\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Select a subset of samples for evaluation\n",
    "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    samples = [dataset[i] for i in indices]\n",
    "    \n",
    "    rouge_l_scores = []\n",
    "    \n",
    "    for sample in tqdm(samples, desc=\"Computing ROUGE-L scores\"):\n",
    "        # Extract question and expected answer\n",
    "        text = sample['formatted_text']\n",
    "        \n",
    "        # Extract question part (before the <think> tag)\n",
    "        question_match = re.search(r\"Question: (.*?)\\n\\n<think>\", text, re.DOTALL)\n",
    "        if not question_match:\n",
    "            continue\n",
    "        question = question_match.group(1).strip()\n",
    "        \n",
    "        # Extract expected answer (inside <response> tags)\n",
    "        answer_match = re.search(r\"<response>(.*?)</response>\", text, re.DOTALL)\n",
    "        if not answer_match:\n",
    "            continue\n",
    "        expected_answer = answer_match.group(1).strip()\n",
    "        \n",
    "        # Generate model's answer\n",
    "        input_text = f\"Question: {question}\\n\\n\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_answer = generated_text[len(input_text):].strip()\n",
    "        \n",
    "        # Compute ROUGE-L score\n",
    "        score = scorer.score(expected_answer, generated_answer)\n",
    "        rouge_l_scores.append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    # Calculate average ROUGE-L score\n",
    "    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0\n",
    "    \n",
    "    return avg_rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the base model before fine-tuning\n",
    "print(\"Evaluating base model performance...\")\n",
    "base_rouge_l = compu
(Content truncated due to size limit. Use line ranges to read in chunks)